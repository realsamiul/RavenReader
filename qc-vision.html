<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>M0NARQ | QC Vision</title>
    <link rel="stylesheet" href="styles.css">
    <style>
        .tech-spec { border-left: 2px solid var(--teal); padding-left: 1rem; margin: 1rem 0; font-family: var(--font-mono); font-size: 0.8rem; color: #aaa; background: rgba(0, 156, 166, 0.05); padding: 1rem; }
        .feature-grid { display: grid; grid-template-columns: repeat(2, 1fr); gap: 2rem; margin-top: 2rem; }
        .feature-item h4 { color: var(--teal); margin-bottom: 0.5rem; }
        .feature-item p { font-size: 0.9rem; color: #ccc; }
    </style>
</head>
<body>
    <div class="preloader"><p class="mono">LOADING MICROVIT QUANTIZED...</p></div>
    
    <button class="nav-island" onclick="toggleMenu()">
        <div class="nav-icon"></div><span class="nav-label">MENU</span>
    </button>
    
    <div class="fullscreen-menu" id="fsMenu">
        <button class="menu-close" onclick="toggleMenu()">X</button>
        <ul class="menu-list">
            <li class="menu-item"><a href="index.html" class="menu-link">M0NARQ</a></li>
            <li class="menu-item"><a href="#" class="menu-link sub">FinancialReader AI</a></li>
            <li class="menu-item"><a href="index.html" class="menu-link sub" style="color:var(--accent); opacity:1;">Industrial AI</a></li>
            <li class="menu-item" style="margin-top:2rem"><a href="qc-vision.html" class="menu-link">01. QC Vision</a></li>
            <li class="menu-item"><a href="power-guard.html" class="menu-link">02. PowerGuard</a></li>
            <li class="menu-item"><a href="fail-predict.html" class="menu-link">03. FailPredict</a></li>
        </ul>
    </div>

    <!-- HERO -->
    <section id="hero" class="bg-black">
        <img src="qc-vision1.webp" class="hero-bg" alt="Fabric Inspection Process">
        <div class="container">
            <span class="module-tag reveal">MODULE 01</span>
            <h1 class="reveal">QC VISION</h1>
            <p class="reveal">Real-time fabric defect detection on commodity edge hardware.<br><span class="teal">28ms Inference. 92% F1 Score.</span></p>
        </div>
    </section>

    <!-- SECTION 1: THE PROBLEM (FATIGUE) -->
    <section class="bg-white section-padding">
        <div class="container">
            <div class="split-layout reveal">
                <div>
                    <h2 style="color:var(--black)">THE HUMAN LIMIT.</h2>
                    <h4 style="color:var(--black); margin-bottom:1rem;">THE 70-40 FATIGUE CURVE</h4>
                    <p style="color:#333">
                        Manual inspection relies on human visual acuity, which degrades rapidly over an 8-12 hour shift. Research indicates that defect capture rates drop from <strong>70%</strong> in the first hour to <strong><40%</strong> by the eighth hour due to cognitive fatigue. This inconsistency leads to a 3-5% defect rate in final output.
                    </p>
                    <p style="color:#333">
                         In a market exporting $32.6B annually, this is a <strong>$390M</strong> opportunity loss. A single "Grade D" roll shipped to a buyer like H&M or Zara can result in a claim of <strong>$3,200</strong>, wiping out the margin for an entire batch.
                    </p>
                     <h4 style="color:var(--black); margin-bottom:1rem; margin-top:2rem;">WHY LEGACY SOLUTIONS FAIL</h4>
                    <p style="color:#333">
                        Imported AOI rigs (e.g., KeyeTech) solve this but cost <strong>$12,000+ per unit</strong>. They are rigid, requiring perfect lighting and massive floor space. They are economically unviable for the 4,000+ SME factories that form the backbone of the supply chain.
                    </p>
                </div>
                <div>
                    <img src="qc-vision2.webp" class="content-image" alt="Defect detection overlay">
                    <div class="stat-grid">
                         <div class="stat-item">
                            <h3 style="color:var(--black)">38%</h3>
                            <p class="mono" style="color:#555">DEFECT REDUCTION</p>
                            <p style="font-size:0.75rem; color:#666">Pilot result: Knit-Dye Plant #3, Gazipur.</p>
                        </div>
                        <div class="stat-item">
                            <h3 style="color:var(--black)">$42k</h3>
                            <p class="mono" style="color:#555">MONTHLY SAVINGS</p>
                            <p style="font-size:0.75rem; color:#666">Material saved per 10k yards daily output.</p>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <!-- SECTION 2: THE SOLUTION (MICROVIT) -->
    <section class="bg-black section-padding">
        <div class="container">
            <h2 class="reveal">THE NEURAL STACK</h2>
            <div class="split-layout reveal">
                <div>
                     <p>We deploy <strong>MicroViT-Tiny-Q8</strong> on recycled smartphones ($20 BOM). Unlike humans, AI does not blink, tire, or miss.</p>
                    <div class="code-wrapper" style="border-left-color: var(--teal);">
<div class="code-block">
<span class="c"># PRODUCTION-GRADE QC PIPELINE</span>
<span class="k">import</span> onnxruntime <span class="k">as</span> ort
<span class="k">import</span> numpy <span class="k">as</span> np

<span class="k">class</span> <span class="f">DefectDetector</span>:
    <span class="k">def</span> <span class="f">__init__</span>(self):
        <span class="c"># 1. MicroViT-Tiny-Q8 (Backbone)</span>
        <span class="c"># 9M Params, int8 Quantized for Edge</span>
        <span class="c"># 3.6x faster than DINO-v2 on ARM CPU</span>
        self.vit = ort.InferenceSession(<span class="s">"/models/microvit_q8.onnx"</span>)
        
        <span class="c"># 2. PicoSAM-2 (Segmentation)</span>
        <span class="c"># Promptable masks for Human-in-the-Loop</span>
        self.sam = ort.InferenceSession(<span class="s">"/models/picosam_v2.onnx"</span>)

    <span class="k">def</span> <span class="f">detect</span>(self, img):
        <span class="c"># Inference: 28ms on Pi 5 CPU</span>
        features = self.vit.run(None, {<span class="s">"input"</span>: img})
        
        <span class="c"># Generate mask using features</span>
        mask = self.sam.run(None, {<span class="s">"embedding"</span>: features})
        
        <span class="c"># Economic Classification Logic</span>
        <span class="k">if</span> np.sum(mask) > <span class="n">5.0</span>:
             <span class="c"># Trigger GPIO to stop machine</span>
            <span class="k">return</span> <span class="s">"STOP_LINE_CRITICAL"</span>
            
        <span class="k">return</span> <span class="s">"LOG_DEFECT"</span>
</div>
                    </div>
                    
                </div>
                <div>
                    <ul style="list-style:none; border-top:1px solid #333; padding-top:2rem">
                        <li style="margin-bottom:2rem">
                            <span class="mono teal">WHY MICROVIT vs DINO-v2?</span>
                            <p>DINO-v2 (Meta) is the gold standard for features but is computationally heavy for a $90 computer. We use <strong>MicroViT-Tiny-Q8</strong>. By quantizing to 8-bit integers, we fit the model into the L2 cache of mobile processors (Snapdragon 778G), achieving a <strong>3.6x speedup</strong> vs ViT-Small while maintaining 91.3% accuracy on fabric texture datasets.</p>
                        </li>
                        <li style="margin-bottom:2rem">
                            <span class="mono teal">PICOSAM: HUMAN-IN-THE-LOOP</span>
                            <p>We replaced the heavy Segment Anything Model (SAM) with <strong>PicoSAM-2</strong> (1.3M params). This allows "Promptable Masks"â€”a floor manager can "tap" a new defect type on a tablet, and the model instantly learns the boundary. This "Few-Shot Learning" adapts to new fabric styles in minutes, not weeks.</p>
                        </li>
                         <li style="margin-bottom:2rem">
                            <span class="mono teal">DEPLOYMENT STEPS</span>
                             <ol style="font-size:0.9rem; color:#aaa; margin-left:1.5rem; margin-top:0.5rem">
                                 <li>Mount Smartphone on Tripod ($20).</li>
                                 <li>Connect via USB to Raspberry Pi 5 ($90).</li>
                                 <li>Run `docker-compose up` to pull MicroViT.</li>
                                 <li>Calibrate lighting (auto-exposure script).</li>
                             </ol>
                        </li>
                    </ul>
                </div>
            </div>
            
            <div class="feature-grid reveal">
                <div class="feature-item">
                     <h4>LATENCY DETERMINISM</h4>
                     <p>Fabric rolls move at high speed. A round trip to the cloud takes 3000ms. Our local inference takes <strong>28ms</strong>. This ensures the machine stops <em>before</em> the defect is wound into the roll.</p>
                </div>
                <div class="feature-item">
                     <h4>PRIVACY BY DESIGN</h4>
                     <p>Proprietary fabric designs and worker faces are processed in RAM and discarded. No images leave the factory unless explicitly flagged for retraining, complying with Bangladesh DSA 2018.</p>
                </div>
            </div>
        </div>
    </section>
    
    <footer class="bg-black section-padding" style="border-top:1px solid #333">
        <div class="container" style="text-align:center">
            <h2 class="mono" style="letter-spacing:0.2em">M0NARQ.AI</h2>
        </div>
    </footer>
    <script src="script.js"></script>
</body>
</html>


