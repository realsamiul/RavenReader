<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>M0NARQ | QC Vision</title>
    <link rel="stylesheet" href="styles.css">
    <style>
        .tech-spec { border-left: 2px solid var(--teal); padding-left: 1rem; margin: 1rem 0; font-family: var(--font-mono); font-size: 0.8rem; color: #aaa; }
    </style>
</head>
<body>
    <div class="preloader"><p class="mono">LOADING MICROVIT QUANTIZED...</p></div>
    
    <button class="nav-island" onclick="toggleMenu()">
        <div class="nav-icon"></div><span class="nav-label">MENU</span>
    </button>
    
    <div class="fullscreen-menu" id="fsMenu">
        <button class="menu-close" onclick="toggleMenu()">X</button>
        <ul class="menu-list">
            <li class="menu-item"><a href="index.html" class="menu-link">M0NARQ</a></li>
            <li class="menu-item"><a href="#" class="menu-link sub">FinancialReader AI</a></li>
            <li class="menu-item"><a href="index.html" class="menu-link sub" style="color:var(--accent); opacity:1;">Industrial AI</a></li>
            <li class="menu-item" style="margin-top:2rem"><a href="qc-vision.html" class="menu-link">01. QC Vision</a></li>
            <li class="menu-item"><a href="power-guard.html" class="menu-link">02. PowerGuard</a></li>
            <li class="menu-item"><a href="fail-predict.html" class="menu-link">03. FailPredict</a></li>
        </ul>
    </div>

    <section id="hero" class="bg-black">
        <img src="qc-vision1.webp" class="hero-bg" alt="Fabric Inspection Process">
        <div class="container">
            <span class="module-tag reveal">MODULE 01</span>
            <h1 class="reveal">QC VISION</h1>
            <p class="reveal">Real-time fabric defect detection on commodity edge hardware.<br><span class="teal">28ms Inference. 92% F1 Score.</span></p>
        </div>
    </section>

    <!-- PROBLEM & SOLUTION -->
    <section class="bg-white section-padding">
        <div class="container">
            <div class="split-layout reveal">
                <div>
                    <h2 style="color:var(--black)">THE HUMAN LIMIT.</h2>
                    <p style="color:#333">
                        Manual inspection relies on human visual acuity, which degrades rapidly over an 8-12 hour shift. Research shows defect capture rates drop from <strong>70%</strong> in the first hour to <strong><40%</strong> by the eighth.
                    </p>
                    <p style="color:#333">
                        <strong>The Economic Leakage:</strong> This inconsistency leads to a 3-5% defect rate. In a market exporting $32.6B annually, this is a <strong>$390M</strong> opportunity loss.
                    </p>
                    <p style="color:#333">
                        <strong>The Solution:</strong> We deploy <strong>MicroViT-Tiny-Q8</strong> on recycled smartphones ($20 BOM). Unlike humans, AI does not blink, tire, or miss.
                    </p>
                </div>
                <div>
                    <img src="qc-vision2.webp" class="content-image" alt="Defect detection overlay">
                    <div class="stat-grid">
                         <div class="stat-item">
                            <h3 style="color:var(--black)">38%</h3>
                            <p class="mono" style="color:#555">DEFECT REDUCTION</p>
                            <p style="font-size:0.75rem; color:#666">Pilot result: Knit-Dye Plant #3, Gazipur.</p>
                        </div>
                        <div class="stat-item">
                            <h3 style="color:var(--black)">$42k</h3>
                            <p class="mono" style="color:#555">MONTHLY SAVINGS</p>
                            <p style="font-size:0.75rem; color:#666">Material saved per 10k yards daily output.</p>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <!-- TECHNICAL DEEP DIVE -->
    <section class="bg-black section-padding">
        <div class="container">
            <h2 class="reveal">TECHNICAL ARCHITECTURE</h2>
            <div class="split-layout reveal">
                <div>
                    <div class="code-wrapper" style="border-left-color: var(--teal);">
<div class="code-block">
<span class="k">@app.cls</span>(gpu=<span class="s">"T4"</span>)
<span class="k">class</span> <span class="f">DefectDetector</span>:
    <span class="k">def</span> <span class="f">__enter__</span>(self):
        <span class="c"># 1. MicroViT-Tiny-Q8 (Backbone)</span>
        <span class="c"># 9M Params, int8 Quantized for Edge</span>
        self.vit = ort.InferenceSession(<span class="s">"/models/microvit_q8.onnx"</span>)
        
        <span class="c"># 2. PicoSAM-2 (Segmentation)</span>
        <span class="c"># Promptable masks for Human-in-the-Loop</span>
        self.sam = load_model(<span class="s">"picosam_v2"</span>)

    <span class="k">def</span> <span class="f">detect</span>(self, img):
        <span class="c"># Inference: 28ms on Pi 5 CPU</span>
        features = self.vit.run(img)
        mask = self.sam.segment(features)
        
        <span class="c"># Economic Classification</span>
        <span class="k">if</span> mask.area > <span class="n">5.0</span>:
            <span class="k">return</span> <span class="s">"STOP_LINE_CRITICAL"</span>
        <span class="k">return</span> <span class="s">"LOG_DEFECT"</span>
</div>
                    </div>
                    <div class="tech-spec">
                        > BACKBONE: MicroViT-Tiny (3.6x faster than DINO-v2)<br>
                        > QUANTIZATION: 8-bit Integer (Run on CPU/NPU)<br>
                        > LATENCY: 28ms (Raspberry Pi 5) / 14ms (Snapdragon)
                    </div>
                </div>
                <div>
                    <ul style="list-style:none; border-top:1px solid #333; padding-top:2rem">
                        <li style="margin-bottom:2rem">
                            <span class="mono teal">WHY NOT DINO-v2?</span>
                            <p>While DINO-v2 is powerful, it is computationally heavy. We use <strong>MicroViT-Tiny-Q8</strong>. By quantizing to 8-bit integers, we fit the model into the limited L2 cache of mobile processors, achieving real-time speeds without the $1000+ cost of an NVIDIA Jetson.</p>
                        </li>
                        <li style="margin-bottom:2rem">
                            <span class="mono teal">HUMAN-IN-THE-LOOP (PicoSAM)</span>
                            <p>We use <strong>PicoSAM-2</strong> for segmentation. It allows a floor manager to "tap" a new defect type on a tablet, instantly creating a mask and updating the local dataset. This "Few-Shot Learning" adapts to new fabric styles in minutes, not weeks.</p>
                        </li>
                    </ul>
                </div>
            </div>
        </div>
    </section>
    
    <footer class="bg-black section-padding" style="border-top:1px solid #333">
        <div class="container" style="text-align:center">
            <h2 class="mono" style="letter-spacing:0.2em">M0NARQ.AI</h2>
        </div>
    </footer>
    <script src="script.js"></script>
</body>
</html>
